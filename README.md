# ğŸš€ Concurrent Data Scraper Bot

**Concurrent_Data_Scraper_Bot** is a **multi-threaded web scraper** designed for high-speed data extraction.  
It uses **BeautifulSoup**, `requests`, and `ThreadPoolExecutor` for efficient concurrent scraping, saving data in **JSON format**.

## ğŸ“Œ Features
- âœ… **Multi-threaded scraping** for high-speed data collection  
- âœ… **Extracts product data** (brand, price, specs, images)  
- âœ… **Saves output in JSON format**  
- âœ… **Handles failed requests & retries automatically**  
- âœ… **Uses `.env` file** to manage environment variables  

## ğŸ› ï¸ Installation & Setup

### ğŸ”¹ 1. Clone the Repository
```sh
git clone https://github.com/keremTheDev/Concurrent_Data_Scraper_Bot.git
cd Concurrent_Data_Scraper_Bot
```
### ğŸ”¹ 2. Install Required Dependencies

```sh
pip install -r requirements.txt
```
### ğŸ”¹ 3. Create & Configure .env File
```sh
LINK_URL=
BASE_URL=
OUTPUT_FILE=
SAVE_FILE=
```
### ğŸ”¹ 4. Run the Scraper
```sh
python main.py
```

## ğŸ› ï¸ How It Works

- 1ï¸âƒ£ The scraper first collects all product links from multiple pages.
- 2ï¸âƒ£ Each product page is processed concurrently using ThreadPoolExecutor.
- 3ï¸âƒ£ Extracted data includes:

- Name, Brand, Price, Stock Status
- Technical specifications (attributes)
- Images and Production Year
- 4ï¸âƒ£ All scraped data is saved in JSON format (ProductsData/LBAllData3.json).

## âš™ï¸ Technologies Used

- Python 3.8+
- BeautifulSoup (for HTML parsing)
- requests (for HTTP requests)
- ThreadPoolExecutor (for concurrent scraping)
- dotenv (for managing environment variables)
- JSON serialization (for structured data storage)